{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "from statistics import mean\n",
    "from typing import Any\n",
    "\n",
    "\n",
    "def _find_project_root() -> Path:\n",
    "    \"\"\"Find repo root whether cwd is repo/ or repo/notebooks/.\"\"\"\n",
    "    cwd = Path.cwd().resolve()\n",
    "    for candidate in (cwd, cwd.parent):\n",
    "        if (candidate / \"viz\" / \"data_archive\").exists():\n",
    "            return candidate\n",
    "    return cwd\n",
    "\n",
    "\n",
    "PROJECT_ROOT = _find_project_root()\n",
    "RUNS_FOLDER = PROJECT_ROOT / \"viz\" / \"data_archive\"\n",
    "\n",
    "# Optional plotting stack (doesn't block the notebook if numpy/pandas are broken)\n",
    "try:\n",
    "    import pandas as pd  # type: ignore\n",
    "    import seaborn as sns  # type: ignore\n",
    "    import matplotlib.pyplot as plt  # type: ignore\n",
    "except Exception:  # pragma: no cover\n",
    "    pd = None  # type: ignore\n",
    "    sns = None  # type: ignore\n",
    "    plt = None  # type: ignore"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "afe5dd522302379a",
   "metadata": {},
   "source": [
    "class Benchmark:\n",
    "    def __init__(\n",
    "        self,\n",
    "        runs_folder: str | Path,\n",
    "        runs_names: list[str],\n",
    "        compare_metrics: list[tuple[str, str, str]],\n",
    "    ) -> None:\n",
    "        self.runs_folder = Path(runs_folder)\n",
    "        self.runs_names = runs_names\n",
    "        self.compare_metrics = compare_metrics\n",
    "\n",
    "    def _discover_files(self, run_name: str) -> list[Path]:\n",
    "        \"\"\"Match env name to files inside runs_folder.\n",
    "\n",
    "        Prefers '*data.json'. Falls back to any '*.json' containing run_name.\n",
    "        Matching is case-insensitive and substring-based.\n",
    "        \"\"\"\n",
    "        run_key = run_name.lower().strip()\n",
    "        data_jsons = sorted(self.runs_folder.glob(\"*data.json\"))\n",
    "        hits = [p for p in data_jsons if run_key in p.name.lower()]\n",
    "        if hits:\n",
    "            return hits\n",
    "\n",
    "        all_jsons = sorted(self.runs_folder.glob(\"*.json\"))\n",
    "        hits = [p for p in all_jsons if run_key in p.name.lower()]\n",
    "        return hits\n",
    "\n",
    "    ALGOS = {\"ppo\", \"sac\", \"transformer\", \"ddpg\", \"rule\"}\n",
    "\n",
    "    @classmethod\n",
    "    def _infer_varied_role(cls, files: list[Path]) -> str | None:\n",
    "        \"\"\"Infer which role is varied by looking at filename tokens.\n",
    "\n",
    "        Example (monopolistic competition):\n",
    "        - ..._gov_us_firm_ppo_...\n",
    "        - ..._gov_us_firm_sac_...\n",
    "        - ..._gov_us_firm_transformer_...\n",
    "\n",
    "        Here the role right before the algo token is 'firm'.\n",
    "        \"\"\"\n",
    "        role_algos: dict[str, set[str]] = {}\n",
    "        for p in files:\n",
    "            tokens = p.stem.lower().split(\"_\")\n",
    "            for i, tok in enumerate(tokens):\n",
    "                if tok in cls.ALGOS and i > 0:\n",
    "                    role = tokens[i - 1]\n",
    "                    role_algos.setdefault(role, set()).add(tok)\n",
    "\n",
    "        if not role_algos:\n",
    "            return None\n",
    "\n",
    "        # choose role with most distinct algos (must be >1 to be \"varied\")\n",
    "        best_role, best_n = None, 1\n",
    "        for role, algos in role_algos.items():\n",
    "            n = len(algos)\n",
    "            if n > best_n:\n",
    "                best_role, best_n = role, n\n",
    "        return best_role\n",
    "\n",
    "    @classmethod\n",
    "    def _label_for_file(cls, p: Path, used: set[str], varied_role: str | None) -> str:\n",
    "        stem = p.stem\n",
    "        tokens = stem.lower().split(\"_\")\n",
    "\n",
    "        label: str | None = None\n",
    "\n",
    "        # If we inferred the varied role, use the token after it as model name.\n",
    "        if varied_role is not None:\n",
    "            for i, tok in enumerate(tokens[:-1]):\n",
    "                if tok == varied_role and tokens[i + 1] in cls.ALGOS:\n",
    "                    label = tokens[i + 1]\n",
    "                    break\n",
    "\n",
    "        # Fallback: common pattern: <env>_..._gov_<ALGO>_...\n",
    "        if label is None and \"_gov_\" in stem:\n",
    "            label = stem.split(\"_gov_\", 1)[1].split(\"_\", 1)[0].lower()\n",
    "\n",
    "        # Fallback: common pattern: <env>_<ALGO>.json\n",
    "        if label is None:\n",
    "            for algo in cls.ALGOS:\n",
    "                if stem.lower().endswith(f\"_{algo}\"):\n",
    "                    label = algo\n",
    "                    break\n",
    "\n",
    "        label = (label or stem).strip() or stem\n",
    "\n",
    "        if label not in used:\n",
    "            used.add(label)\n",
    "            return label\n",
    "\n",
    "        # Disambiguate duplicates\n",
    "        i = 2\n",
    "        while f\"{label}#{i}\" in used:\n",
    "            i += 1\n",
    "        label2 = f\"{label}#{i}\"\n",
    "        used.add(label2)\n",
    "        return label2\n",
    "\n",
    "    @staticmethod\n",
    "    def _to_number(x: Any) -> float:\n",
    "        \"\"\"Convert scalars or nested lists/tuples of scalars to a single float (sum).\"\"\"\n",
    "        if isinstance(x, (list, tuple)):\n",
    "            if len(x) == 0:\n",
    "                return 0.0\n",
    "            if len(x) == 1:\n",
    "                return Benchmark._to_number(x[0])\n",
    "            return float(sum(Benchmark._to_number(v) for v in x))\n",
    "        return float(x)\n",
    "\n",
    "    @staticmethod\n",
    "    def _flatten_numbers(x: Any) -> list[float]:\n",
    "        \"\"\"Return all numeric leaves from a nested list/tuple structure.\"\"\"\n",
    "        if isinstance(x, (list, tuple)):\n",
    "            out: list[float] = []\n",
    "            for v in x:\n",
    "                out.extend(Benchmark._flatten_numbers(v))\n",
    "            return out\n",
    "        try:\n",
    "            return [float(x)]\n",
    "        except Exception:\n",
    "            return []\n",
    "\n",
    "    @staticmethod\n",
    "    def _agg(values: list[float], how: str) -> float:\n",
    "        how = how.lower().strip()\n",
    "        if not values:\n",
    "            return float(\"nan\")\n",
    "        if how == \"sum\":\n",
    "            return float(sum(values))\n",
    "        if how in {\"avg\", \"mean\"}:\n",
    "            return float(mean(values))\n",
    "        if how == \"last\":\n",
    "            return float(values[-1])\n",
    "        raise ValueError(f\"Unknown aggregation: {how!r}\")\n",
    "\n",
    "    def _calc_metric(self, run_data: dict[str, Any], metric_key: str, how: str) -> float | None:\n",
    "        if metric_key not in run_data:\n",
    "            return None\n",
    "\n",
    "        raw = run_data[metric_key]\n",
    "        how_norm = how.lower().strip()\n",
    "\n",
    "        # For deeply nested arrays (e.g. house_income: years x households x [value])\n",
    "        # compute mean over ALL numeric leaves.\n",
    "        if how_norm in {\"leaf_avg\", \"avg_leaf\", \"mean_leaf\"}:\n",
    "            leaves = self._flatten_numbers(raw)\n",
    "            return float(mean(leaves)) if leaves else float(\"nan\")\n",
    "\n",
    "        if how_norm in {\"leaf_sum\", \"sum_leaf\"}:\n",
    "            leaves = self._flatten_numbers(raw)\n",
    "            return float(sum(leaves)) if leaves else float(\"nan\")\n",
    "\n",
    "        # list of (possibly list-wrapped) numbers: [[1.0], [0.2], ...]\n",
    "        if isinstance(raw, list):\n",
    "            values = [self._to_number(v) for v in raw]\n",
    "            return self._agg(values, how_norm)\n",
    "\n",
    "        # scalar\n",
    "        try:\n",
    "            return self._to_number(raw)\n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "    def run(self) -> dict[str, dict[str, dict[str, float | None]]]:\n",
    "        \"\"\"Run metrics for all requested environments.\n",
    "\n",
    "        Returns: results[env][metric_out_name][label] = value_or_None\n",
    "        \"\"\"\n",
    "        results: dict[str, dict[str, dict[str, float | None]]] = {}\n",
    "\n",
    "        for env in self.runs_names:\n",
    "            files = self._discover_files(env)\n",
    "            results[env] = {out: {} for out, _, _ in self.compare_metrics}\n",
    "\n",
    "            print(\"=\" * 40)\n",
    "            print(env)\n",
    "            if not files:\n",
    "                print(f\"No files matched in {self.runs_folder}\")\n",
    "                continue\n",
    "            print(f\"Matched {len(files)} file(s)\")\n",
    "\n",
    "            varied_role = self._infer_varied_role(files)\n",
    "            if varied_role is not None:\n",
    "                print(f\"Role: {varied_role}\")\n",
    "\n",
    "            used_labels: set[str] = set()\n",
    "            for p in files:\n",
    "                label = self._label_for_file(p, used_labels, varied_role)\n",
    "                try:\n",
    "                    run_data = json.loads(p.read_text())\n",
    "                except Exception as e:\n",
    "                    print(f\"- {label}: failed to read {p.name}: {e}\")\n",
    "                    for out_name, _, _ in self.compare_metrics:\n",
    "                        results[env][out_name][label] = None\n",
    "                    continue\n",
    "\n",
    "                if not isinstance(run_data, dict):\n",
    "                    print(f\"- {label}: unsupported JSON shape ({type(run_data)}) in {p.name}\")\n",
    "                    for out_name, _, _ in self.compare_metrics:\n",
    "                        results[env][out_name][label] = None\n",
    "                    continue\n",
    "\n",
    "                for out_name, metric_key, how in self.compare_metrics:\n",
    "                    results[env][out_name][label] = self._calc_metric(run_data, metric_key, how)\n",
    "\n",
    "            # Pretty-print\n",
    "            for out_name, _, _ in self.compare_metrics:\n",
    "                print(\"---------------------\")\n",
    "                print(out_name)\n",
    "                for label, val in results[env][out_name].items():\n",
    "                    print(f\"{label}: {val if val is not None else 'N/A'}\")\n",
    "\n",
    "        return results"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3646f9330a409778",
   "metadata": {},
   "source": [
    "bench = Benchmark(\n",
    "    runs_folder=RUNS_FOLDER,\n",
    "    runs_names=[\n",
    "        \"estate_tax\",\n",
    "        \"monopolistic_competition\",\n",
    "        \"inflation_control\",\n",
    "        \"optimal_monetary_gov\",\n",
    "        \"optimal_monetary_bank\",\n",
    "        \"optimal_tax\",\n",
    "        \"oligopoly\",\n",
    "        \"delayed_retirement\",\n",
    "        \"work_hard\",\n",
    "        \"work_life_well_being\",\n",
    "        \"pension_gap\",\n",
    "        \"universal_basic_income\",\n",
    "    ],\n",
    "    compare_metrics=[\n",
    "        (\"accumulative_gov_reward\", \"gov_reward\", \"sum\"),\n",
    "        (\"accumulative_firm_reward\", \"firm_reward\", \"sum\"),\n",
    "        (\"accumulative_bank_reward\", \"bank_reward\", \"sum\"),\n",
    "        (\"social_welfare_avg\", \"social_welfare\", \"avg\"),\n",
    "        (\"wealth_gini_avg\", \"wealth_gini\", \"avg\"),\n",
    "        (\"gdp\", \"GDP\", \"avg\"),\n",
    "        # house_income format: years x households x [value]\n",
    "        (\"house_income_avg\", \"house_income\", \"leaf_avg\"),\n",
    "    ],\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e573606d8b3784db",
   "metadata": {},
   "source": [
    "results = bench.run()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "edad93c4977e0aac",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3d15cf8f904ee75e",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
